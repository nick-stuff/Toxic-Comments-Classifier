{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafb72dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54600d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa311a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860eea20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54329c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58da0245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e090d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b139d5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#I obtained the frequency distributions of different parts of speech from this cell. This is optional to read.\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from collections import Counter\n",
    "from nltk import FreqDist\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from string import punctuation\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score,f1_score\n",
    "\n",
    "# \"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"\n",
    "\n",
    "# read dataset from file\n",
    "scoredComments = []\n",
    "scores= []\n",
    "\n",
    "\n",
    "scoredCommentsTokenized = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "dataset = pd.read_csv(\"train.csv\")\n",
    "\n",
    "\n",
    "#display(dataset)\n",
    "#print(dataset.describe())\n",
    "\n",
    "\n",
    "toxicSum = dataset[\"toxic\"].sum()\n",
    "sevToxicSum = dataset[\"severe_toxic\"].sum()\n",
    "obsceneSum = dataset[\"obscene\"].sum()\n",
    "threatSum = dataset[\"threat\"].sum()\n",
    "insSum = dataset[\"insult\"].sum()\n",
    "idhateSum = dataset[\"identity_hate\"].sum()\n",
    "\n",
    "total = 159570\n",
    "print(\"Toxic rate:\" + str(toxicSum / 159570))\n",
    "print(\"Severe Toxic rate:\" + str(sevToxicSum / 159570))\n",
    "print(\"Obscene rate:\" + str(obsceneSum/ 159570))\n",
    "print(\"Threat rate:\" + str(threatSum / 159570))\n",
    "print(\"Insult rate:\" + str(insSum / 159570))\n",
    "print(\"Identity Hate rate:\" + str(idhateSum/ 159570))\n",
    "\n",
    "\n",
    "\n",
    "y = [toxicSum,sevToxicSum,obsceneSum,threatSum,insSum,idhateSum]\n",
    "labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "plt.suptitle('Distribution of Categories')\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.barplot(labels,y)\n",
    "\n",
    "def evaluate(clf, Xtrain,Xtest, Ytrain,Ytest,comment):\n",
    "    name = clf.__class__.__name__.split('.')[-1]\n",
    "    resultList=[]\n",
    "    for label in labels:\n",
    "        clf.fit(Xtrain, Ytrain[label])\n",
    "        y_prediction = clf.predict(Xtest)\n",
    "        recallScore = recall_score(Ytest[label], y_prediction, average= 'weighted')\n",
    "        f1 = f1_score(Ytest[label], y_prediction, average= 'weighted')\n",
    "        result = (name, label, recallScore,f1)\n",
    "        resultList.append(result)\n",
    "    \n",
    "    results = pd.DataFrame(resultList, columns=['name', 'category', 'recallScore', 'f1'])\n",
    "    display(results)\n",
    "\n",
    "def crossVal(clf, X,y):\n",
    "    name = clf.__class__.__name__.split('.')[-1]\n",
    "    resultList=[]\n",
    "    for label in labels:\n",
    "        score = cross_val_score(clf, X, y[label], cv=5, scoring='recall')\n",
    "        f1 = cross_val_score(clf, X, y[label], cv=5, scoring='f1')\n",
    "        result = (name, label, score.mean(), f1.mean())\n",
    "        resultList.append(result)\n",
    "    \n",
    "    results = pd.DataFrame(resultList, columns=['name', 'category', 'recallScore', 'f1'])\n",
    "    results = results.style.set_caption('Cross Validation')    \n",
    "    display(results)    \n",
    "    \n",
    "        \n",
    "\n",
    "dataset = pd.DataFrame()\n",
    "toxicWords = []\n",
    "nonToxicWords = []\n",
    "hateWords =[]\n",
    "\n",
    "\n",
    "with open(\"train.csv\", 'r',  encoding=\"utf8\") as file:\n",
    "    next(file)\n",
    "    csvreader= csv.reader(file, delimiter=',')\n",
    "    rowcounter = 0\n",
    "    for row in csvreader:\n",
    "        rowcounter+=1\n",
    "        if rowcounter > 30000:\n",
    "            break\n",
    "       \n",
    "        \n",
    "        comment = row[1]\n",
    "        commLength = len(comment)\n",
    "        comment = re.sub('\\s+', ' ', comment)  \n",
    "        comment = re.sub(r'[^\\x00-\\x7F]',' ', comment)\n",
    "        comment = re.sub(r'\\d', '',comment) \n",
    "        comment = re.sub(r\"\\n\", \" \", comment)\n",
    "        \n",
    "        punctuation = re.sub(\"[A-Za-z ]\", \"\", comment)\n",
    "        punctuationRate = len(punctuation) / commLength\n",
    "        \n",
    "        capitals = ''.join([c for c in comment if c.isupper()])\n",
    "        capitalRate = len(capitals)/ commLength\n",
    "      \n",
    "        \n",
    "        comment = re.sub(r'[^\\w\\s]','',comment)\n",
    "        \n",
    "        \n",
    "        \n",
    "        comment = comment.lower()\n",
    "\n",
    "        toxics = int(row[2])\n",
    "        severe_toxic = int(row[3])\n",
    "        obscene = int(row[4])\n",
    "        threat = int(row[5])\n",
    "        insult = int(row[6])\n",
    "        identity_hate = int(row[7])\n",
    "        \n",
    "\n",
    "        \n",
    "        score = (toxics,severe_toxic,obscene,threat,insult,identity_hate)\n",
    "        wordList = []    \n",
    "        words = word_tokenize(comment)\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in stop_words and word != '':\n",
    "                word = lemma.lemmatize(word)\n",
    "                wordList.append(word)\n",
    "       \n",
    "        tags = nltk.pos_tag(words)\n",
    "        \n",
    "        tagDict = {}\n",
    "        \n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        funWord = []\n",
    "        adverbs = []\n",
    "        adjectives = []\n",
    "        \n",
    "        tagNames = [\"nouns, verbs, funWord, adverbs, adjectives\"]\n",
    "        \n",
    "        for word in tags:\n",
    "            if(word[1][0] == \"N\"):\n",
    "                nouns.append(word[0])\n",
    "            elif(word[1][0] == \"V\"):\n",
    "                verbs.append(word[0])\n",
    "            elif(word[1] == \"DT\" or word[1] == \"WDT\" or word[1] == \"PDT\" or word[1] == \"CC\" or word[1] == \"WP\" or word[1] == \"TO\" or word[1] == \"EX\" or word[1] == \"IN\"):\n",
    "                funWord.append(word[0])\n",
    "            elif (word[1][0] == \"R\" or word[1] == \"WRB\"):\n",
    "                adverbs.append(word[0])\n",
    "            elif (word[1][0] == \"J\"):\n",
    "                adjectives.append(word[0])\n",
    "       \n",
    "        nounsFreq=0\n",
    "        verbsFreq=0\n",
    "        funWordFreq=0\n",
    "        adverbsFreq=0\n",
    "        adjectivesFreq=0\n",
    "        \n",
    "        if (len(wordList) > 0):\n",
    "            nounsFreq = len(nouns)/len(wordList)\n",
    "            verbsFreq = len(verbs)/len(wordList)\n",
    "            funWordFreq = len(funWord)/len(wordList)\n",
    "            adverbsFreq = len(adverbs)/len(wordList)\n",
    "            adjectivesFreq = len(adjectives)/len(wordList)\n",
    "        summation = 0\n",
    "        for cat in score:\n",
    "            summation+=cat\n",
    "        if (summation == 0):\n",
    "            nonToxicWords.extend(wordList)\n",
    "        if(toxics == 1):\n",
    "            toxicWords.extend(wordList)\n",
    "        if(identity_hate == 1):\n",
    "            hateWords.extend(wordList)\n",
    "        \n",
    "        \n",
    "        #print(nounsFreq)\n",
    "        scoredComment = (' '.join(wordList), nounsFreq , verbsFreq, funWordFreq, adverbsFreq, adjectivesFreq, capitalRate,  punctuationRate)\n",
    "        \n",
    "        scoredComments.append(scoredComment)\n",
    "        scores.append(np.array(score))\n",
    "\n",
    "fdist = FreqDist(toxicWords)\n",
    "print(fdist.most_common(50))\n",
    "\n",
    "fdist = FreqDist(hateWords)\n",
    "print(fdist.most_common(50))\n",
    "fdist = FreqDist( nonToxicWords)\n",
    "print(fdist.most_common(50))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "dataset = pd.DataFrame(scoredComments, columns=['comments', 'noun freq', 'verb freq','func word freq', 'adverb freq', 'adjective freq', 'capital freq', 'punctuation freq'])\n",
    "\n",
    "scoreFrame = pd.DataFrame(scores, columns=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"])\n",
    "\n",
    "\n",
    "\n",
    "dataset= pd.concat([dataset,scoreFrame], ignore_index=True, axis =1)\n",
    "dataset.columns = ['comments', 'noun freq', 'verb freq','func word freq', 'adverb freq', 'adjective freq', 'capital freq', 'punctuation freq', \"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "#display(dataset)\n",
    "\n",
    "\n",
    "\n",
    "col_list = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "\n",
    "nonToxicLen = len(dataset[dataset[col_list].sum(axis=1) == 0])\n",
    "\n",
    "\n",
    "nonToxicSum = dataset.loc[dataset[col_list].sum(axis=1) == 0, 'capital freq'].sum()\n",
    "nonToxicCapitalRate = nonToxicSum/nonToxicLen\n",
    "capitalRateList = [nonToxicCapitalRate]\n",
    "\n",
    "nonToxicpuncSum= dataset.loc[dataset[col_list].sum(axis=1) == 0, 'punctuation freq'].sum()\n",
    "nonToxicPunRate = nonToxicpuncSum/nonToxicLen\n",
    "puncRateList = [nonToxicPunRate]\n",
    "\n",
    "nounSum= dataset.loc[dataset[col_list].sum(axis=1) == 0, 'noun freq'].sum()\n",
    "nounRate = nounSum/nonToxicLen\n",
    "nounRateList = [nounRate]\n",
    "\n",
    "verbSum= dataset.loc[dataset[col_list].sum(axis=1) == 0, 'verb freq'].sum()\n",
    "verbRate = verbSum/nonToxicLen\n",
    "verbRateList = [verbRate]\n",
    "\n",
    "funcSum= dataset.loc[dataset[col_list].sum(axis=1) == 0, 'func word freq'].sum()\n",
    "funcRate = funcSum/nonToxicLen\n",
    "funcRateList = [funcRate]\n",
    "\n",
    "adverbSum= dataset.loc[dataset[col_list].sum(axis=1) == 0, 'adverb freq'].sum()\n",
    "adverbRate = adverbSum/nonToxicLen\n",
    "adverbRateList = [adverbRate]\n",
    "\n",
    "adjectiveSum= dataset.loc[dataset[col_list].sum(axis=1) == 0, 'adjective freq'].sum()\n",
    "adjectiveRate = adjectiveSum/nonToxicLen\n",
    "adjectiveRateList = [adjectiveRate]\n",
    "\n",
    "\n",
    "for label in labels:\n",
    "   \n",
    "    sumCapitals = dataset.loc[dataset[label] == 1, 'capital freq'].sum()\n",
    "    length = len(dataset[dataset[label] == 1])\n",
    "    capitalsRate = sumCapitals/length\n",
    "    capitalRateList.append(capitalsRate)\n",
    " \n",
    "    puncSum= dataset.loc[dataset[label] == 1, 'punctuation freq'].sum()\n",
    "    puncRate = puncSum/length\n",
    "    puncRateList.append(puncRate)\n",
    "\n",
    "    nounSum= dataset.loc[dataset[label] == 1, 'noun freq'].sum()\n",
    "   \n",
    "    nounRate = nounSum/length\n",
    "    nounRateList.append(nounRate)\n",
    "\n",
    "    \n",
    "    verbSum= dataset.loc[dataset[label]== 1, 'verb freq'].sum()\n",
    "    #print(verbSum)\n",
    "    verbRate = verbSum/length\n",
    "    #print(verbRate)\n",
    "    \n",
    "    verbRateList.append(verbRate)\n",
    "\n",
    "    funcSum= dataset.loc[dataset[label]== 1, 'func word freq'].sum()\n",
    "    funcRate = funcSum/length\n",
    "    funcRateList.append(funcRate)\n",
    "\n",
    "    adverbSum= dataset.loc[dataset[label] == 1, 'adverb freq'].sum()\n",
    "    adverbRate = adverbSum/length\n",
    "    adverbRateList.append(adverbRate)\n",
    "\n",
    "    adjectiveSum= dataset.loc[dataset[label] == 1, 'adjective freq'].sum()\n",
    "    adjectiveRate = adjectiveSum/length\n",
    "    adjectiveRateList.append(adjectiveRate)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "y = capitalRateList\n",
    "\n",
    "labels = [\"non toxic\", \"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "plt.figure(figsize = (10,5))\n",
    "plt.suptitle('Capitals Rate')\n",
    "sns.barplot(labels,y)\n",
    "\n",
    "y = puncRateList\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle('Punctuation Rate')\n",
    "sns.barplot(labels,y)\n",
    "plt.show()\n",
    "\n",
    "y = nounRateList\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle('Noun Rate')\n",
    "sns.barplot(labels,y)\n",
    "plt.show()\n",
    "\n",
    "y = verbRateList\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle('Verb Rate')\n",
    "plot = sns.barplot(labels,y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "y = funcRateList\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle('Function Word Rate')\n",
    "sns.barplot(labels,y)\n",
    "plt.show()\n",
    "\n",
    "y = adverbRateList\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle('Adverb Rate')\n",
    "sns.barplot(labels,y)\n",
    "plt.show()\n",
    "\n",
    "y = adjectiveRateList\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.suptitle('Adjective Rate')\n",
    "sns.barplot(labels,y)\n",
    "plt.show()\n",
    "\n",
    "####OPTIONAL CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcd2701",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc9b66",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####TFIDF VECTORIZATION \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from string import punctuation\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score,f1_score\n",
    "\n",
    "# \"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"\n",
    "\n",
    "scoredComments = []\n",
    "scores= []\n",
    "\n",
    "tweets = []\n",
    "scoredCommentsTokenized = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "dataset = pd.read_csv(\"train.csv\")\n",
    "\n",
    "\n",
    "display(dataset)\n",
    "\n",
    "print(dataset.describe())\n",
    "\n",
    "\n",
    "toxicSum = dataset[\"toxic\"].sum()\n",
    "sevToxicSum = dataset[\"severe_toxic\"].sum()\n",
    "obsceneSum = dataset[\"obscene\"].sum()\n",
    "threatSum = dataset[\"threat\"].sum()\n",
    "insSum = dataset[\"insult\"].sum()\n",
    "idhateSum = dataset[\"identity_hate\"].sum()\n",
    "\n",
    "total = 159570\n",
    "print(\"Toxic rate:\" + str(toxicSum / 159570))\n",
    "print(\"Severe Toxic rate:\" + str(sevToxicSum / 159570))\n",
    "print(\"Obscene rate:\" + str(obsceneSum/ 159570))\n",
    "print(\"Threat rate:\" + str(threatSum / 159570))\n",
    "print(\"Insult rate:\" + str(insSum / 159570))\n",
    "print(\"Identity Hate rate:\" + str(idhateSum/ 159570))\n",
    "\n",
    "\n",
    "\n",
    "y = [toxicSum,sevToxicSum,obsceneSum,threatSum,insSum,idhateSum]\n",
    "labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "plt.figure(figsize = (10,5))\n",
    "sns.barplot(labels,y)\n",
    "\n",
    "def evaluate(clf, Xtrain,Xtest, Ytrain,Ytest):\n",
    "    \n",
    "    name = clf.__class__.__name__.split('.')[-1]\n",
    "    resultList=[]\n",
    "    for label in labels:\n",
    "        clf.fit(Xtrain, Ytrain[label])\n",
    "        y_prediction = clf.predict(Xtest)\n",
    "        recallScore = recall_score(Ytest[label], y_prediction, average= 'weighted')\n",
    "        f1 = f1_score(Ytest[label], y_prediction, average= 'weighted')\n",
    "        result = (name, label, recallScore,f1)\n",
    "        resultList.append(result)\n",
    "    \n",
    "    \n",
    "    results = pd.DataFrame(resultList, columns=['name', 'category', 'recallScore', 'f1'])\n",
    "    MeanRecall = results['recallScore'].sum()/6\n",
    "    MeanF1 = results['f1'].sum()/6\n",
    "    #results = pd.concat([results, (name, \"Mean Recall and f1\", MeanRecall, MeanF1)])\n",
    "    \n",
    "    l = {'name':name, 'category':\"Mean Recall and f1\", 'recallScore':MeanRecall, 'f1':MeanF1}\n",
    "   \n",
    "    results = results.append(l, ignore_index = True,)\n",
    "    \n",
    "    display(results)\n",
    "\n",
    "def crossVal(clf, X,y):\n",
    "    name = clf.__class__.__name__.split('.')[-1]\n",
    "    resultList=[]\n",
    "    for label in labels:\n",
    "        score = cross_val_score(clf, X, y[label], cv=5, scoring='recall')\n",
    "        f1 = cross_val_score(clf, X, y[label], cv=5, scoring='f1')\n",
    "        result = (name, label, score.mean(), f1.mean())\n",
    "        resultList.append(result)\n",
    "    \n",
    "    results = pd.DataFrame(resultList, columns=['name', 'category', 'recallScore', 'f1'])\n",
    "    \n",
    "    \n",
    "    MeanRecall = results['recallScore'].sum()/6\n",
    "    MeanF1 = results['f1'].sum()/6\n",
    "    \n",
    "    l = {'name':name, 'category':\"Mean Recall and f1\", 'recallScore':MeanRecall, 'f1':MeanF1}\n",
    "    results = results.append(l, ignore_index = True,)\n",
    "    \n",
    "    results = results.style.set_caption('Cross Validation')    \n",
    "    display(results)\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "#dataset = pd.DataFrame()\n",
    "\n",
    "with open(\"train.csv\", 'r',  encoding=\"utf8\") as file:\n",
    "    next(file)\n",
    "    #skip first entry (name of categories)\n",
    "    csvreader= csv.reader(file, delimiter=',')\n",
    "    rowcounter = 0\n",
    "    #iterate through csv\n",
    "    for row in csvreader:\n",
    "        rowcounter+=1\n",
    "        if rowcounter > 30000:\n",
    "            break        \n",
    "        #we are only interested in the first 30000 entries\n",
    "    \n",
    "        comment = row[1].lower()\n",
    "        comment = re.sub('\\s+', ' ', comment)  \n",
    "        comment = re.sub(r'[^\\x00-\\x7F]','', comment)\n",
    "        comment = re.sub(r'\\d', '',comment) \n",
    "        comment = re.sub(r\"\\n\", \" \", comment)\n",
    "        comment = re.sub(r'[^\\w\\s]','',comment)\n",
    "        #cleaning document (converted to lowercase)\n",
    "        #removing punctuation, replacing multiple spaces with one space\n",
    "        #removing non-ascii chars, newline escape chars, digits\n",
    "    \n",
    "        \n",
    "        toxics = int(row[2])\n",
    "        severe_toxic = int(row[3])\n",
    "        obscene = int(row[4])\n",
    "        threat = int(row[5])\n",
    "        insult = int(row[6])\n",
    "        identity_hate = int(row[7])\n",
    "        #getting scores from csv\n",
    "        \n",
    "        score = (toxics,severe_toxic,obscene,threat,insult,identity_hate)\n",
    "        #tuple containing scores\n",
    "        \n",
    "        wordList = []    \n",
    "        words = word_tokenize(comment)\n",
    "        #tokenize comment into words\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                word = lemma.lemmatize(word)\n",
    "                wordList.append(word)\n",
    "        #removing stop words and lemmatizing words.\n",
    "        \n",
    "        scoredComments.append(' '.join(wordList))\n",
    "        scores.append(np.array(score))\n",
    "        \n",
    "        #rejoin list of words (for use in the tfidf vectorizer) and add it to cleaned comments list\n",
    "        #add comment's score (categories) to list of scores \n",
    "\n",
    "        \n",
    "#create dataframe for comments and scores\n",
    "dataset = pd.DataFrame(scoredComments, columns=['comments'])\n",
    "scoreFrame = pd.DataFrame(scores, columns=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"])\n",
    "\n",
    "#concatenate dataframes\n",
    "dataset= pd.concat([dataset,scoreFrame], ignore_index=True, axis =1)\n",
    "dataset.columns = ['comments',\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "\n",
    "display(dataset)\n",
    "\n",
    "tfIdfvec = TfidfVectorizer(use_idf=True, max_features = 10000, min_df = 10, max_df = 0.60,  ngram_range = (1,1))\n",
    "X, y = train_test_split(dataset, test_size=0.2,shuffle=True)\n",
    "\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))\n",
    "\n",
    "X_train_vectors_tfidf = tfIdfvec.fit_transform(X[\"comments\"])\n",
    "X_test_vectors_tfidf = tfIdfvec.transform(y[\"comments\"])\n",
    "X_eval_vectors_tfidf = tfIdfvec.fit_transform(dataset[\"comments\"])\n",
    "\n",
    "clf = LinearSVC()\n",
    "evaluate(clf, X_train_vectors_tfidf, X_test_vectors_tfidf, X, y)\n",
    "crossVal(clf,X_eval_vectors_tfidf, dataset)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "evaluate(clf, X_train_vectors_tfidf, X_test_vectors_tfidf, X, y)\n",
    "crossVal(clf,X_eval_vectors_tfidf, dataset)\n",
    "\n",
    "clf =LogisticRegression()\n",
    "evaluate(clf, X_train_vectors_tfidf, X_test_vectors_tfidf, X, y)\n",
    "crossVal(clf,X_eval_vectors_tfidf, dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5ae765",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8165246a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ba8e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf1dfce",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####Word2Vec Vectorization\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import scale\n",
    "from time import time\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import multiprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from string import punctuation\n",
    "import csv\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.svm import LinearSVC\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import recall_score,f1_score\n",
    "\n",
    "# \"id\",\"comment_text\",\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"\n",
    "\n",
    "# read dataset from file\n",
    "scoredComments = []\n",
    "scores= []\n",
    "\n",
    "tweets = []\n",
    "scoredCommentsTokenized = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "dataset = pd.read_csv(\"train.csv\")\n",
    "\n",
    "labels = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
    "def evaluate(clf, Xtrain,Xtest, Ytrain,Ytest):\n",
    "    name = clf.__class__.__name__.split('.')[-1]\n",
    "    resultList=[]\n",
    "    for label in labels:\n",
    "        clf.fit(Xtrain, Ytrain[label])\n",
    "        \n",
    "        y_prediction = clf.predict(Xtest)\n",
    "\n",
    "        recallScore = recall_score(Ytest[label], y_prediction, average= 'weighted')\n",
    "        f1 = f1_score(Ytest[label], y_prediction, average= 'weighted')\n",
    "        result = (name, label, recallScore,f1)\n",
    "        resultList.append(result)\n",
    "    \n",
    "    results = pd.DataFrame(resultList, columns=['name', 'category', 'recallScore', 'f1'])\n",
    "    MeanRecall = results['recallScore'].sum()/6\n",
    "    MeanF1 = results['f1'].sum()/6\n",
    "    \n",
    "    l = {'name':name, 'category':\"Mean Recall and f1\", 'recallScore':MeanRecall, 'f1':MeanF1}\n",
    "    results = results.append(l, ignore_index = True,)\n",
    "    \n",
    "    display(results)\n",
    "\n",
    "def crossVal(clf, X,y):\n",
    "    name = clf.__class__.__name__.split('.')[-1]\n",
    "    resultList=[]\n",
    "    for label in labels:\n",
    "        score = cross_val_score(clf, X, y[label], cv=5, scoring='recall')\n",
    "        f1 = cross_val_score(clf, X, y[label], cv=5, scoring='f1')\n",
    "        result = (name, label, score.mean(), f1.mean())\n",
    "        resultList.append(result)\n",
    "    \n",
    "    results = pd.DataFrame(resultList, columns=['name', 'category', 'recallScore', 'f1'])\n",
    "    \n",
    "    MeanRecall = results['recallScore'].sum()/6\n",
    "    MeanF1 = results['f1'].sum()/6\n",
    "    \n",
    "    l = {'name':name, 'category':\"Mean Recall and f1\", 'recallScore':MeanRecall, 'f1':MeanF1}\n",
    "    results = results.append(l, ignore_index = True,)\n",
    "    \n",
    "    results = results.style.set_caption('Cross Validation')    \n",
    "    display(results)\n",
    "    \n",
    "    \n",
    "tfidfList = []\n",
    "dataset = pd.DataFrame()\n",
    "\n",
    "with open(\"train.csv\", 'r',  encoding=\"utf8\") as file:\n",
    "    next(file)\n",
    "    csvreader= csv.reader(file, delimiter=',')\n",
    "    rowcounter = 0\n",
    "    #for i in range(40000): next(csvreader)\n",
    "    for row in csvreader:\n",
    "        rowcounter+=1\n",
    "        if rowcounter > 30000:\n",
    "            break\n",
    "            \n",
    "        comment = row[1].lower()\n",
    "        comment = re.sub(r'[^\\x00-\\x7F]',' ', comment)\n",
    "        comment = re.sub(r'\\d', ' ',comment) \n",
    "        comment = re.sub(r\"\\n\", \" \", comment)\n",
    "        comment = re.sub(r'[^\\w\\s]',' ',comment)\n",
    "        comment = re.sub('\\s+', ' ', comment)  \n",
    "        \n",
    "        \n",
    "        toxics = int(row[2])\n",
    "        severe_toxic = int(row[3])\n",
    "        obscene = int(row[4])\n",
    "        threat = int(row[5])\n",
    "        insult = int(row[6])\n",
    "        identity_hate = int(row[7])\n",
    "        \n",
    "        score = (toxics,severe_toxic,obscene,threat,insult,identity_hate)\n",
    "        wordList = []    \n",
    "        words = word_tokenize(comment)\n",
    "        \n",
    "        for word in words:\n",
    "            if word not in stop_words:\n",
    "                word = lemma.lemmatize(word)\n",
    "                wordList.append(word)\n",
    "        \n",
    "        scoredComments.append(wordList)\n",
    "        scores.append(np.array(score))\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "\n",
    "#placing the scores into a dataframe\n",
    "scoreFrame = pd.DataFrame(scores, columns=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"])\n",
    "\n",
    "#initilizaing the Word2Vec model\n",
    "w2v_model = Word2Vec(scoredComments,\n",
    "                     workers = cores-1,\n",
    "                     min_count=20,\n",
    "                     window=2,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     vector_size = 300,\n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20)\n",
    "\n",
    "def word_vector(tokens, size):\n",
    "    featureVec = np.zeros((size,), dtype=\"float32\")\n",
    "    count = 0\n",
    "    word_index = set(w2v_model.wv.index_to_key)\n",
    "    \n",
    "    for word in tokens:\n",
    "        if word in word_index:\n",
    "            count+=1\n",
    "            score = w2v_model.wv.get_vector(word) \n",
    "            featureVec = np.add(featureVec, score)\n",
    "    if count > 0:      \n",
    "        featureVec = np.divide(featureVec, count)\n",
    "    return(featureVec)\n",
    "    \n",
    "\n",
    "vectorList = []\n",
    "#get vectors for each comment\n",
    "for entry in scoredComments:\n",
    "    vector = word_vector(entry, 300)\n",
    "    vectorList.append(vector)\n",
    "    \n",
    "#store vectors in dataframe\n",
    "vectorFrame = pd.DataFrame(vectorList)\n",
    "display(vectorFrame)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(vectorFrame, scoreFrame, test_size=0.2,shuffle=True)\n",
    "\n",
    "clf = LinearSVC(max_iter = 4000)\n",
    "evaluate(clf, X_train, X_test, y_train, y_test)\n",
    "crossVal(clf,vectorFrame, scoreFrame)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "evaluate(clf, X_train, X_test, y_train, y_test)\n",
    "crossVal(clf,vectorFrame, scoreFrame)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "evaluate(clf, X_train, X_test, y_train, y_test)\n",
    "crossVal(clf,vectorFrame, scoreFrame)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f14a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a431e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
